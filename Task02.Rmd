---
title: "SCEM Final Coursework"
subtitle: "Task II"
author: "Rohan Anthony (2704500)"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

*****

## IMPORTANT NOTES: 
- **DO NOT** change the code block names. Enter your solutions to each question into the predefined code blocks. 
- **Don't forget to replace the placeholder value in code block "Setup" by your valid student ID number.**
- If you want to use any packages, add their names to the vector `required.packages` in the code block "Setup". That code block will 
automatically install those packages (if needed) and load them. Any other calls to `install.packages()`, `library()` or `require()` may result in zero marks for the activity where it happens.
- Make sure that the data files *df.rds* and *df_holdout.rds* are located in the same folder as this file, _Task02.Rmd_.
- Instructions for this task are provided in blue text below. Your solution should NOT be added within those text boxes, and should NOT be in blue text. **During marking, all the text within the text blocks below, with names starting with "prompt", will be removed and not visible to the marker.**

```{r ID, echo=FALSE, message=FALSE, warning=FALSE}
## We will use your student ID as the seed for the random number generator.
MY_STUDENT_ID <- 2704500 # <-- Replace "ABCDEFG" by your student ID number (as an integer, i.e., without quotes).
```

```{r Setup, echo=FALSE, message=FALSE, warning=FALSE}
force_reinstall_everything <- FALSE ## <--- change to TRUE to force a reinstall of 
                                    ## all packages listed in allowed.packages

## ADD ANY REQUIRED PACKAGES TO THE VECTOR BELOW
## Note: tidyverse includes:  ggplot2, dplyr, tidyr, readr, purrr, tibble, 
##                            stringr, forcats, and lubridate.
allowed.packages <- c("tidyverse", 
                      "tidymodels", 
                      "recipes", 
                      "parsnip", 
                      "bonsai", 
                      "healthyR.ai", 
                      "knitr",
                      "ranger")


## DO NOT CHANGE ANYTHING ELSE IN THIS CODE BLOCK
for (i in seq_along(allowed.packages)){
  if(!(allowed.packages[i] %in% installed.packages()[, 1]) || force_reinstall_everything){
    install.packages(allowed.packages[i], 
                     dependencies = c("Depends", "Imports"),
                     repos = "https://cloud.r-project.org")
  }
  library(allowed.packages[i], character.only = TRUE)
}

knitr::opts_chunk$set(echo = TRUE, collapse=TRUE)
ignore <- runif(1) # To initialise the PRNG

cat("\nList of allowed packages (pre-loaded):\n ", paste(allowed.packages, collapse="\n  "))
```

```{r checkID, echo=FALSE}
if(!is.numeric(MY_STUDENT_ID)){
  stop("MY_STUDENT_ID MUST BE YOUR VALID NUMERIC ID.")
} 
```

*****

:::{#prompt .message style="color: blue;"}
In this task you will build a full predictive pipeline for the problem described in the coursework specs. Please read each activity carefully and follow the instructions to complete your coursework.

Make sure that the data file **df.rds** is placed in the same folder as this Task02.Rmd file. The data that you can use for model development in this task will then be automatically loaded and set up for you, and stored into a dataframe variable called `df`.
:::


```{r load_data, echo = FALSE}
## DO NOT CHANGE ANYTHING IN THIS CODE BLOCK
set.seed(MY_STUDENT_ID)
df <- readRDS("df.rds") %>% sample_frac(0.6, replace = FALSE)
```

*****

## 1. Exploratory Data Analysis
:::{#prompt .message style="color: blue;"}
**Note**: It is expected that this section should result in no more than 600-ish words (possibly less; not counting the code blocks).

Add here your exploratory data analysis. Your solution should include both your code (which markers must be able to run independently) and your rationale for each exploratory step. In general, your EDA steps should include:

- A short justification of what a given plot or statistical summary is intended to reveal about the data.

- A code block implementing your EDA step.

At the end of your EDA section, you should also include a short commentary on what your data exploration revealed about the data, and which pre-processing steps may be required based on that.

You can use as many code blocks as needed, but don't overdo it (remember - everything needs to have a proper rationale). 
:::
```{r eda_step1}
# EDA Step 1: Overview of data set, structure and size
cat("Observations (Rows):", nrow(df), "\n")
cat("Variables (Columns):", ncol(df), "\n")

glimpse(df)

# Categorizing columns based on prefixes in Task 2 Context
prefix_counts <- tibble(name = names(df)) %>%
  mutate(prefix = case_when(
    str_starts(name, "Info_") ~ "Info_",
    str_starts(name, "feat_") ~ "feat_",
    name == "Class" ~ "Class",
    TRUE ~ "other"
  )) %>%
  count(prefix, sort = TRUE)
prefix_counts
```

The dataset consists of 7403 observations and 390 variables. Of these, 385 are numerical feature columns (prefix 'feat_'), 4 information columns (prefix 'Info_') and one target variable 'Class'.

```{r eda_step2}
# EDA Step 2: Checking class distribution and labels
class_tab <- df %>%
  count(Class) %>%
  arrange(desc(n)) %>%
  mutate(prop = n / sum(n))
print(class_tab)

# Converting Class to a factor
df <- df %>%
  mutate(Class = factor(Class))

```

The target variable Class is moderately balanced with approximately 59% of observations in class '-1' and 41% in class '1'. It has been converted to a factor to ensure proper model handling.

```{r eda_step3}
# EDA Step 3: Checking for missing values

# Total number of missing values in the dataset
total_missing <- sum(is.na(df))
total_missing

# Count missing values for each variable
missing_summary <- df %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "missing") %>%
  filter(missing > 0) %>%
  arrange(desc(missing))

missing_summary

# Identifying if there are any missing values in Class or any of the Info_ variables
"Class" %in% missing_summary$variable

any(str_starts(missing_summary$variable, "Info_"))

missing_summary %>%
  filter(str_starts(variable, "Info_"))

```
A total of 486 missing values were found across the dataset. Missingness is sparse because this is much smaller than the total number of cells in the dataset and no individual variable contains more than 5 missing values. We have also verified that there are no missing values in the target variable 'Class' and only one 'Info_' column contains a missing value. Given the low and unstructured nature of the missing values, imputation will be sufficient during preprocessing.

```{r eda_step4}
# EDA Step 4: Scale, variance, outliers and correlation of numeric features

# Summarizing min and max values for each numerical feature
df_num <- df %>% select(starts_with("feat_"))

scale_summary <- df_num %>%
  summarise(across(everything(),
                   list(min = ~min(.x, na.rm = TRUE),
                        max = ~max(.x, na.rm = TRUE)))) %>%
  pivot_longer(everything(),
               names_to = c("variable", "stat"),
               names_pattern = "(.*)_(min|max)$") %>%
  pivot_wider(names_from = stat, values_from = value)

scale_summary %>%
  ggplot(aes(x = min, y = max)) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Feature Scale (Min vs Max)",
       x = "Min",
       y = "Max")

# Determining global min and max values across all numerical features
all_vals   <- unlist(df_num)
global_min <- min(all_vals, na.rm = TRUE)
global_max <- max(all_vals, na.rm = TRUE)

cat("Global numeric feature range: [",global_min, ", ",global_max,"]\n")

# Checking for variables with near zero variance
nzv_check <- df_num %>%
  summarise(across(everything(), ~ length(unique(.)))) %>%
  pivot_longer(cols = everything(),
               names_to = "feature",
               values_to = "n_unique") %>%
  arrange(n_unique)

head(nzv_check)

# Checking for extreme values per feature for outliers using z-score
outlier_check <- df_num %>%
  summarise(across(
    everything(),
    ~ mean(abs(scale(.x)) > 3, na.rm = TRUE)
  )) %>%
  pivot_longer(everything(),
               names_to = "feature",
               values_to = "prop_outliers") %>%
  arrange(desc(prop_outliers))

summary(outlier_check$prop_outliers)

# Checking for correlation between numerical features
corr_mat <- cor(df_num, use = "pairwise.complete.obs")
corr_df <- as.data.frame(as.table(corr_mat)) %>% setNames(c("Var1","Var2","Corr")) %>%
  filter(Var1 != Var2) %>% mutate(AbsCorr = abs(Corr)) %>% arrange(desc(AbsCorr))

cat("Top correlated pairs (abs corr):\n")
print(head(corr_df, 10))


```
We can see that the numerical features vary widely in scale. Most features have min and max values clustered near zero but some take very large positive values and some take negative values. Due to the high dimensionality of the dataset, we have used a min-max scatter plot to check the scale of values. Over all values the global range is approximately [-152, 3.3e5]. These extreme values suggest that scaling will be required. From our near zero variance check, we can see that all features exhibit substantial variability, therefore we do not need to filter out any of these features in further steps.

To detect outliers we computed the proportion of observations with Z score magnitude greater than 3 for each numerical feature. Most features have between 0% and 2.5% such values which indicates some extreme observations but only in small proportions. The correlation check shows several pairs of highly correlated features which will be addressed by PCA during modelling.

Overall, the EDA shows that the dataset is high-dimensional with a moderately balanced target variable. Missing values are sparse, so simple imputation will be sufficient. The numerical features exhibit substantial difference in scale, so scaling will be required before modelling. All features show adequate variability, so we do not need any feature filtering. The proportion of extreme values is small, so outlier removal is unnecessary.

*****

## 2. Data preprocessing and feature engineering

:::{#prompt .message style="color: blue;"}
**Note**: It is expected that this section should result in no more than 400-ish words (possibly less; not counting the code blocks). 

Add here your data pre-processing and feature selection/engineering steps. Your solution should include both your code (which markers must be able to run independently) and your rationale. In general, your steps should include:

- A short justification of which insight from your EDA is motivating a given data transformation.

- A code block implementing your pre-processing step.

**In addition** to the regular data transformations emerging from the insights you gathered during EDA, this section must contain the  splitting off of a test set that you'll use later for final performance assessment  (check the CW specs for details).

At the end of this section you should also include a short commentary on what changes the pre-processing has induced in your data (i.e., what did the data "look like" before this step vs. what it "looks like" afterwards.

You can use as many code blocks as needed, but don't overdo it (remember - everything needs to have a proper rationale). If you compare different pre-processing options, make sure to make it clear, and to indicate at the end which option you selected, and why. 

**Note**: In this section you'll be basically defining which data transformations you will later incorporate into your predictive pipeline. When you build the pipeline in the next section, you'll encapsulate all those steps (together with the model) into a single object that you can fit and deploy as a single unit.
:::

We first split the data into train and test splits by group on Info_group to prevent data leakage. 
```{r dpp_step1}
# DPP Step 1: Splitting data into train and test sets by group
set.seed(MY_STUDENT_ID)

# 80:20 split into training and test based on Info_group
data_split <- group_initial_split(df, group = Info_group, prop = 0.8)

df_train <- training(data_split)
df_test <- testing(data_split)

length(intersect(df_train$Info_group, df_test$Info_group))

```

In the construction of our recipe we note from EDA:
1. Info_ columns are non informative as predictors and should be excluded from the predictor set.
2. Missing values are sparse and unstructured, so trimmed mean imputation is sufficient.
3. Numerical features have very different scales so we apply normalization.
4. Small proportion of outliers across features tells us that Winsorisation is not necessary.

```{r dpp_step2}
# DPP Step 2: Preprocessing recipe based on EDA

pp_recipe <- recipe(Class ~ ., data = df_train) %>%
  step_rm(starts_with("Info_")) %>%
  step_impute_mean(all_numeric_predictors(), trim = 0.1) %>%
  step_normalize(all_numeric_predictors())
pp_recipe

pp_recipe_prepped <- prep(pp_recipe)
pp_recipe_prepped

train_processed <- bake(pp_recipe_prepped, df_train)
train_processed

cat("Info_ columns ",
    any(startsWith(names(train_processed), "Info_")), "\n")
cat("Total remaining NAs after preprocessing:",
    sum(is.na(train_processed)), "\n")
cat("Number of predictors after preprocessing (excluding Class):",
    ncol(train_processed) - 1, "\n")

```
The preprocessing recipe removes all non-informative Info_ columns from the predictor set, imputes missing values using winsorised mean (10% trim) and normalizes all numeric features. The only unaddressed insight from EDA is the presence of highly correlated features, which we will address during modelling using PCA. 

*****

## 3. Modelling

:::{#prompt .message style="color: blue;"}
**Note**: It is expected that this section should result in no more than 400-ish words (possibly less; not counting the code blocks).

Add here your modelling steps. Your solution should include both your code (which markers must be able to run independently) and your rationale. This section must include:

- The fitting of a preliminary model on your preprocessed training data (to estimate some baseline performance level and to make sure your data is ready for modelling). Make this a simple model - logistic regression, decision trees or kNN are good options (but other may be used instead, if you prefer).

- The building of a predictive pipeline encapsulating all the learned/learnable aspects of your solution. This should result in a pipeline-type object that uses your raw training data (post EDA and test set splitting, but prior to any preprocessing), fits all your preprocessing transformations (incl. dimensionality reduction) and the predictive model using that data, and can then be used to ingest your test data (or any new data in the same format) and generate predictions for each entry. 

Please refer to the CW specs for other requirements of this step (including, e.g., the addition of a dimensionality reduction step into your pipeline).

You can use as many code blocks as needed, but don't overdo it (remember - everything needs to have a proper rationale). If you compare different models, make sure to make it clear, and to indicate at the end which pipeline option is your final selected one.
:::
```{r modelling_step1}
# Modelling Step 1: Simple logistic regression using DPP recipe
lr_model <- logistic_reg() %>%
  set_mode("classification") %>%
  set_engine("glm")

lr_wf <- workflow() %>%
  add_recipe(pp_recipe) %>%
  add_model(lr_model)

lr_fit <- lr_wf %>% fit(df_train)
lr_fit
```
We have fit a preliminary predictive model based on logistic regression to ensure that our data is ready for modelling. From this model, we see warnings of fitted probabilities, NA coefficients and extremely large weights. These are indicative of severe multicollinearity. This aligns with our EDA and confirms that dimensionality reduction via PCA is required. 

```{r modelling_step2}
# Modelling Step 2: PCA dimensionality reduction.
pca_recipe <- pp_recipe %>%
  step_pca(all_numeric_predictors(), num_comp = tune())
pca_recipe
```
From our EDA and preliminary model, we learned that PCA is required and has been added to the recipe. PCA will transform our 385 correlated features into a smaller set of uncorrelated principal components. We have set this number of components to be tuned as a hyperparameter.

```{r modelling_step3}
# Modelling Step 3a: Building Random Forest with PCA
rf_model <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 500
) %>%
  set_mode("classification") %>%
  set_engine("ranger")

rf_wf <- workflow() %>%
  add_recipe(pca_recipe) %>%
  add_model(rf_model)
rf_wf

# Modelling Step 3b: Building Elastic Net Logistic Regression with PCA
enet_model <- logistic_reg(
  penalty = tune(),
  mixture = tune()
) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

enet_wf <- workflow() %>%
  add_recipe(pca_recipe) %>%
  add_model(enet_model)
enet_wf
```
We have selected two candidate sophisticated models to test and decide on the better model. The selected models are Random Forest and Elastic Net Logistic Regression. They were selected due to their ability to work on high dimensional data.

```{r modelling_step4}
# Modelling Step 4: Grouped cross-validation to prevent data leakage
set.seed(MY_STUDENT_ID)

group_folds <- group_vfold_cv(
  df_train,
  group = Info_group,
  v = 5
)

group_folds
```
To prevent data leakage we have used a 5-fold cross-validation, where folds are constructed using 'Info_group'.

```{r modelling_step5}
# Modelling Step 5: Comparing Random Forest and Elastic Net

# Simple PCA recipe without tuning
simple_pca_recipe <- pp_recipe %>%
  step_pca(all_numeric_predictors(), num_comp = 30)

# 5a Simple Random Forest
rf_model_simple <- rand_forest(
  mtry  = 15,
  min_n = 5,
  trees = 500
) %>%
  set_mode("classification") %>%
  set_engine("ranger")

rf_wf_simple <- workflow() %>%
  add_recipe(simple_pca_recipe) %>%
  add_model(rf_model_simple)

set.seed(MY_STUDENT_ID)
rf_cv <- fit_resamples(
  rf_wf_simple,
  resamples = group_folds,
  metrics = metric_set(f_meas)
)

rf_results <- collect_metrics(rf_cv)
rf_results

# 5b) Simple Elastic Net
enet_model_simple <- logistic_reg(
  penalty = 0.01,
  mixture = 0.5
) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

enet_wf_simple <- workflow() %>%
  add_recipe(simple_pca_recipe) %>%
  add_model(enet_model_simple)

set.seed(MY_STUDENT_ID)
enet_cv <- fit_resamples(
  enet_wf_simple,
  resamples = group_folds,
  metrics = metric_set(f_meas)
)

enet_results <- collect_metrics(enet_cv)
enet_results
```
On comparing Random Forest and Elastic Net, we see that Random Forest has an F1 score of approximately 0.767 while Elastic Net has an F1 score of approximately 0.739. This tells us that Random Forest performs better than Elastic Net and is selected as our model for next steps.

*****

## 4. Model assessment

:::{#prompt .message style="color: blue;"}
**Note**: It is expected that this section should result in no more than 100-ish words (possibly less; not counting the code blocks).

This is a short section in which you must estimate the generalisation performance of your model on the test set that you isolated at the start of your Data preprocessing step. It is also a chance for you to ensure that your pipeline object can take in new data and return the required predictions.

Use your pipeline to generate predictions for your test set, and calculate the observed performance using the $F_1$-score as your main performance indicator. You can also additionally report other performance metrics if you think it's relevant.

You can use as many code blocks as needed, but it's likely that this section can be completed with a single one. Make sure that your performance indicators are clearly echoed, so that a reader can easily check if after re-running your script. Add a short comment on the observed performance (is it good? Is it poor? Remember that you're not being assessed on the performance of your models, but on the soundness of your approach).
:::

*****

## 5. Model deployment

:::{#prompt .message style="color: blue;"}
**Note**: This section is not expected to have much text in it, only the code and the results.

Finally, in this section you'll generate predictions for some data for which you do not know the class labels. This simulates the real-life scenario in which your model needs to be deployed to generate new predictions for new, unknown data (after all, if the labels were known, we wouldn't need a model).

Make sure that the data file __df_holdout.rds__ is placed in the same folder as this Task02.Rmd file. The new data will then be automatically loaded and set up for you, and stored into a dataframe variable called `df_holdout`, with the same columns as the original `df` that was loaded at the start of this task.

- Use your fitted pipeline to generate predictions for this new data.

- Build a new data frame called containing the following columns:
    - `Info_PepID` (taken from df_holdout)
    - `Info_pos`  (taken from df_holdout)
    - `Predicted_class` (with the predictions produced by your pipeline)

- Save your predictions dataframe to a file called __mypreds.rds__. You will be able to submit this file, and it will be used to give you some feedback on the actual performance of your pipeline on new data.
:::

```{r load_holdout, echo = FALSE}
## DO NOT CHANGE ANYTHING IN THIS CODE BLOCK
df_holdout <- readRDS("df_holdout.rds")
```

*****
*****

### =========== End of Task II ===========
